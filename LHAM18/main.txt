\title{
Performance evaluation and visualization of scientific applications using PMlib
}

%	Abstractも内容を修正する必要があるかもしれない

\begin{abstract}
Sustained computational performance of scientific applications on HPC systems
is often observed much lower than the maximum system performance.
Understanding this gap requires multi-perspective analysis, i.e., the system
architecture perspective to evaluate the characteristics of micro architecture
elements such as processor core and memory, and the applications perspective
to correlate the theoretical computation coded as the source program with the
computation workload produced by compilers.
The authors developed PMlib open source library to address such synthetic analysis.
PMlib provides the way to report the arithmetic workload counted manually from
the source code, as well as the actually executed system workload.
It also provides the detail utilization report of processor specific hardware
including the categorized SIMD instruction statistics, the layered cache
hit/miss rate, and the corresponding bandwidth up to memory,
which are captured via hardware performance counters (HWPC).
Using of PMlib, users can conduct synthetic analysis of the application
performance, and obtain the hint for further optimized execution of applications.
\end{abstract}


\section{Introduction}

The purpose of this research.
In the development and the productive phase of the scientific application,
the performance evaluation is frequently conducted on the target HPC systems.
in order to understand the performance characteristics
of the application on the target system, and to explore the possibility of
applying further optimization to the software for elapsed time reduction.

As the background, there is well known fact that 
the sustained computational performance of scientific applications on HPC
systems can be much lower than the theoretical maximum performance
expected from the system specification, depending on the type of applications.

\cite{cite-1-1}

There have been many prededing studies to account for this gap between
the achieved performance and the maximum performance, mostly from architecture
point of view,
some mentioning the difficulty of utilizing parallel arithmetic functional units,
some mentioning the locality of data residing on memory/cache hiererchy.

\cite{paper-bottleneck-analysis}

The performance is measured in the unit of conducted workload / elapsed time.
We should remind that the definition of workload measurement can vary,
usually chosen from one of the the followings,
the arithmetic workload defined by the applications in their source program,
and the system workload actually executed on HPC systems that is represented as
machine instructions which is measured with hardware performance counters (HWPC).
The difference in the performance based on these two workload definitions
can be significant as explained in the later section \ref{},
and is sometimes confusing to application users.

%	Obvious relational effect of job manager and the system workload is not
%	the focus of this paper.

PMlib has the functionality
to explicitly measure the arithmetic workload counted in the source program
using manually formulated argument, as well as the functionality
to automatically measure the HWPC event base workload,
thus providing users an option to compare the difference in workloads.

In measuring the HWPC events, PMlib utilizes PAPI \cite{} low level API.
PMlib has its own scheme to choose the related HWPC event sets and to sort out
the event statistics according to run time environment variables, making it
easier to extract the statistics of interest.
Choosing the correct set of events for specific processor type and
translating the accumulated counts into meaningful performance index can be
non-trivial work if without PMlib.

The output information from PMlib is a blocked text report based on
the time averaged performance statistics, by default.
It also produces the Open Trace Format \cite{} tracing file reflecting
the shorter interval statistics which can be read by Web browser
visualization tool TRAiL \cite{}.

In this paper, some examples of the difference between
the arithmetic workload and the system workload will be shown first.

Next, moving on to the performance measurement of the workload,
some examples of PMlib feature to read the performance characteristics
will be shown.

Finally, some examples of the performance analysis and optimization
will be shown indicating the merit obtained from 
multi-perspective analysis.

%	detecting the performance bottleneck ???

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Computational workload and Performance evaluation}
\subsection{user perspective performance}

	\color{blue}{arithmetic/application/system workload} \\
	\color{blue}{Choose the terminology!!!} \\

The term "computational workload" for scientific application developpers
is generally perceived as the total volume of arithmetic operations,
expressed by the formulas in the source code written in Fortran/C/C++.

The workload at this level can be simply represented as below:
%	\begin{math}
%	\end{math}
%単純結合式%	Use Sigma symbol to simplify
\begin{align*}
workload_{arithmetic} = N_{add} + N_{sub} + N_{mult} + N_{div} \\
	+ N_{max} + N_{sqrt} + ...
\end{align*}


For example, the performance value "flops" reported from Linpack (HPCC)
\cite{}
is calculated based on above arithmetic workload formula
\ref{}
using terms of addition and multiplication only.

\begin{align*}
Gflops = N^{2} \times ( 2/3 * N + 3/2 ) \times 1.0^{-9} / Time 
\end{align*}


In the development phase of the HPC applications, practical developers
choose and deploy the numerical algorithm which require the least
amount of arithmetic operations through the scientific consideration,
aiming to reduce the elapsed time to obtain the desired simulation results.
The workload in this context is named arithmetic workload in this paper.

\color{blue}{PMlib user mode}
PMlib can be used to evaluate this arithmetic workload as explained
in the later section \ref{}.




the source code formulas are then compiled to corresponding assembly
instructions.
Each arithmetic statement is mapped to a sequence of multiple instructions.
We use "weight factor" to map the original arithmetic operations to the
generated instructions to quantify the relationship.
Assuming a simple "weight factor" constant w_* for each type of
arithmetic operations, 
the workload at this point can be represented as below:

% workload_{system}
\begin{align*}
%単純結合式%	Use Sigma symbol to simplify
workload_{weight} =
	w_{add}\times N_{add} + w_{sub}\times N_{sub} + w_{mult}\times N_{mult} \\
	+ w_{div}\times N_{div} + w_{max}\times N_{max} + w_{sqrt}\times N_{sqrt} + ...
\end{align*}

\begin{align*}
Performance = workload / Time 
\end{align*}

This workload formula is named application workload in this paper.
The application workload formula is a simple representation of the original
arithmetic and provides easy-to-understand relationship between the change
in the algorithm and the resulting change in the workload.

Under certain conditions
%	namely, the compiler choice and its optimization options,
PMlib can be used to help formulate this application workload as explained
in the later section \ref{}.

While formulating the workload in the form of
\ref{}
or
\ref{}
provides the general idea of the theoretical workload requirement by the
application, it is open difficult for casual applications to formulate
the workload. The other representation of workload will be shown in
the subsequent section.



\subsection{system perspective performance}

For many applications, it can be difficult to formulate
the application workload.
Even if composing the formula is possible, the mapping of "weight factor"
can be even more complex ,as described below.

Mapping mathematical functions to the native instructions
depends on not only the systems architecture but also the compiling software
and its optimization options.
%	mostly, the compiler choice and its optimization options,

There are multiple choices of available instructions for the same
arithmetic operation on modern processors.
The choice of instructions from scalar/vector syntax is made by
compiler optimization strategy, for example.
In vector, the degree of parallelism, i.e. width of simultaneous data
operation, with the effect of loop length is influential.
In scalar, data locality is influential.
Some of run time hardware behavior including out-of-order execution can not
be predicted as "weight factor".

Now we look the workload and the performance from system perspective.

The fact that the actual numerical computation is accomplished by the
limited set of hardware components, and that the HWPC is available
on modern HPC systems, makes it more practical to obtain the HWPC statistics
and to analyze them for performance evaluation.

reflects the combination of the latency and the throughput of the instruction
of the particular application.

PMlib can be used for such purpose.

%
% Can add example of:
%	"arithmetic workload" from the source code
%	"application performance" read from assembly code
%	PMlib measured "HWPC performance"
%

%%%%%%%%%%%%%   Start from here... %%%%%%%%%%%%%%%

\subsection{micro architecture elements}
In system architecture perspective,
micro architecture elements, above all, the number of compute cores
in the processor, the degree of parallelism inside the core,
the depth of cache/memory hierarchy and the data move rate at each hierarchy
all have relational impact to how the machine instructions are executed,
thus leading to sustained computing performance.

parallel



\subsection{PMlib addresses it!}

It also provides the processor specific hardware utilization report
including categorized SIMD instruction statistics,
layered cache hit/miss rate, and the corresponding bandwidth upto memory.
It utilizes PAPI low level API for obtaining HWPC event statistics.

\subsection{conclusion}
Through the use of PMlib, users can conduct synthetic analysis of the
application performance characteristics, and obtain the hint for
further optimized execution of the application.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	以下は IJSP163 日本語
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection {システム評価的観点での性能}
\label{subsection:system-perf}


Through appropriate APIs, HWPC statistics can be accessed. PMlib utilize
PAPI \cite{papi-1} low level API to accomplish this, and calculate the
computing workload from the executed instructions.
For example, Intel Xeon Skylake Gold \cite{skylake-1} processor
floating point operations can be derived from the HWPC event statistics
using the formula below. \ref{skylake-ops-1}


\begin{math}
fpsp1  : "FP\_ARITH:SCALAR\_SINGLE"; \\
fpsp4  : "FP\_ARITH:128B\_PACKED\_SINGLE"; \\
fpsp8  : "FP\_ARITH:256B\_PACKED\_SINGLE"; \\
fpsp16 : "FP\_ARITH:512B\_PACKED\_SINGLE"; \\
fpdp1  : "FP\_ARITH:SCALAR\_DOUBLE"; \\
fpdp2  : "FP\_ARITH:128B\_PACKED\_DOUBLE"; \\
fpdp4  : "FP\_ARITH:256B\_PACKED\_DOUBLE"; \\
fpdp8  : "FP\_ARITH:512B\_PACKED\_DOUBLE";
\end{math}
\begin{align*}
OP{hwpc} = fpsp1 + fpdp1 + 4.0*fpsp4 + 8.0*fpsp8 \\
	+ 16.0*fpsp16 + 2.0*fpdp2 + 4.0*fpdp4 + 8.0*fpdp8;
\end{align*}

Fujitsu FX100 processor, for example, has a different event symbols as below.
\par
\begin{math}
fpdp1  : "1FLOPS\_INSTRUCTIONS"; \\
fpdp2  : "2FLOPS\_INSTRUCTIONS"; \\
fpdp4  : "4FLOPS\_INSTRUCTIONS"; \\
fpdp8  : "8FLOPS\_INSTRUCTIONS"; \\
fpdp16 : "16FLOPS\_INSTRUCTIONS";
\end{math}
\begin{align*}
OP{hwpc} = fpdp1 + 2.0*fpdp2 + 4.0*fpdp4 \\
	+ 8.0*fpdp8 + 16.0*fpdp16; 
\end{align*}

We call the workload measured in this fashion as HWPC workload,
and the evaluated performance as HWPC performance.
HWPC performance is frequently used in the 


The situation that the arithmetic performance, i.e. user perspective,
of scientific applications on HPC systems is low,
even if the HWPC performance, i.e. system perspective, is high
actually exhibits the difference of the workload definition as described above.


\section{PMlib}

\subsection {about PMlib}

PMlib is an open source library to monitor the scientific applications
performance.
\cite{PMlib2:webpage} \par

PMlib can measure two types of workload, i.e. arithmetic and HWPC.
Users insert the PMlib start/stop API statements in the source code.
Each measuring section has minimal properties such as name, type of operation,
exclusiveness, and the workload value.
The report from PMlib is classified into threads, processes, sections,
depending on the controlling environment variable.


\subsection {PMlib API}
PMlib API can be called from C++ and Fortran programs as below.

%\ref{listing1}
% \begin{lstlisting}[caption={FortranプログラムへのPMlib組み込み例}]
% \begin{lstlisting}[caption={\hfill},label={listing1},captionpos=t]
\begin{lstlisting}
! Fortran example
program main
call f_pm_initialize (Nsections)
call f_pm_setproperties ("section1" icalc, iexcl)
call f_pm_start ("section1")
call MyComputation (fops)
call f_pm_stop ("section1", fops, ncall)
call f_pm_print ("", isort)
call f_pm_printdetail ("", ilegend, isort)
end
\end{lstlisting}


\subsection {choosing workload type}
If the arithmetic workload must be measured,
PMlib API pair start/stop accepts argument holding the value or the formula
of the workload contained inside the section sandwiched by start/stop.
The API accumulates the volume of workload internally towards its report.

If HWPC workload must be measured,
PMlib automatically detects the type of hardware, and reads the HWPC
event statistics at each of start/stop API.
In HWPC workload mode, PMlib does not use the argument value, if given any.
The choice of arithmetic workload or HWPC workload is made by a
run time environment variable HWPC\_CHOOSER.

Possible choice of HWPC\_CHOOSER value:

FLOPS \textbar BANDWIDTH\textbar VECTOR\textbar CACHE

While PAPI has a good set of user APIs and is well documented,
it is not a simple task to choose the correct native event sets and masks
of the specific target HPC processor type and to sort out the event values for
a desired performance category using a limited set of HWPC.
For example,
the standard output from papi\_native\_avail command on a server with
Intel Skylake Gold processor gives 5000+ lines.

PMlib takes the role to interface the application developer's intent to
categorize the performance with the raw information from HWPC, and to
help evaluate the performance.


Once PMlib APIs are coded in the application, PMlib preserves the HWPC
portability. That is, PMlib chooses and reads the appropriate HWPC event
set, if the application is run on different HPC systems.


%	PMlib report example 1


%	PMlib report example 2


%	PMlib report example 3



\subsection{PMlib internal timer}
PMlib is a portable open source package, and utilizes linux standard timer
gettimeofday by default. It also has the installation option to utilize
a system specific high resolution low overhead timer for several platforms.
The two HPC systems in the previous section have such timer, and PMlib has
provision to use them.
Fig \ref{fig:precise-timer} shows the comparison of standard timer and
high resolution timer on these system.


\begin{figure}[bt]
\centering\includegraphics[width=0.45\textwidth]{figs/precise-timer.png}
\caption{precise-timer}
\label{fig:precise-timer}
\end{figure}



\subsection{PMlib output information}

{\color{blue} Maybe duplicated???}

The output information from PMlib is a blocked text report based on
the time averaged performance statistics, by default.
It also produces the Open Trace Format \cite{} tracing file reflecting
the shorter interval statistics which can be read by Web browser
visualization tool TRAiL \cite{}.


\subsection{other performance evaluation tools and related research}

There has been various tools developed for HPC system  performance evaluation.

In the open source category:
\begin{itemize}
	\item Scalasca \cite{Scalasca:2017} : trace generation, Score-P common
	infrastructure
	\item Extrae \cite{Extrae:webpage} :  trace generation
	\item PAPI \cite{PAPI:5.6} : API to access HWPC
	\item Linux perf tools : API to access HWPC
\end{itemize}

In the vendor supplied X86 category:
\begin{itemize}
		\item Intel VTune \cite{Intel:VTune}, PGI Profiler \cite{PGI:Profiler}
\end{itemize}

HPC systems also have the performance evaluation tools provided by the
system vendors.
Each of the tools has its own cons and pros.
The system vendor provided tools are integrated and qualified in general,
but is available on the vendor's HPC systems only.

The open source tools are designed to be portable. The functionality of the
tools are distinct, and multiple tools are used in sequence to obtain
the desired performance information.

These existing tools all utilize the processor performance information
based on HWPC workload, i.e. system workload.
PMlib appears to be the only tool that enables arithmetic workload
evaluation.



\section{PMlib use case}


Some examples of using PMlib for performance evaluation are shown in this
section.
The servers used for the measurements and their processor specifications
are listed in table \ref{tab:server-config}.

\subsection{measured HPC systems}

\begin{itemize}
{
%	\setlength{\itemsep}{-5pt}
%	\setlength{\topsep}{2mm}
\item SGI Intel Ivybridge server
\item SGI Intel Skylake server
\item Fujitsu prime HPC FX100
}
\end{itemize}


\newif\ifTwoservers
\newif\ifThreeservers
\Twoserversfalse
\Threeserverstrue
%\tiny
%\footnotesize
%\small
\begin{table}[b]
\scriptsize
\caption{server configuration and maximum performance}
\label{tab:server-config}
\footnotesize

\ifTwoservers
\begin{tabular}{l|c|c} \hline
\scriptsize
system			&	FX100	&	Skylake	\\ \hline
CPU				&	SPARC64 XIfx	&	Gold 6148	\\ \hline
core GHz		&	1.975	&	2.4	\\ \hline
core Gflops	&	31.6	&	〜30	\\ \hline
L1\$ size (D,I)		&	64KB, 64KB	&	32KB, 32KB	\\ \hline
L1D\$ BW GB/s	&	140/R+70/W	&	154/R + 77/W	\\ \hline
\$ Linesize 	&	256B	&	64B	\\ \hline
L2\$ size		&	-	&	1MB	\\ \hline
L2\$ BW GB/s/core	&	-	&	154 ( ~70)	\\ \hline
LL\$ size		&	12MB	&	28MB(1.4MB/c)	\\ \hline
LL\$ BW GB/s/core	&	70/R+35/W	&	77 ( ~43)	\\ \hline
Memory			&	HMC(8x16Ls)	&	DDR4-2666	\\ \hline
Mem GB/s/[CMGcpu]	&	120/R+120/W	&	128	\\ \hline
\#cores/[CMGcpu]	&	16	&	20	\\ \hline
\end{tabular}
\fi

\ifThreeservers
\begin{tabular}{l|c|c|c} \hline
\scriptsize
Symbol			&	FX100	&	SKY		&	IVY \\ \hline
Platform		&	FX100	&	Skylake & Ivybridge\\ \hline
CPU				&	SPARC64 XIfx	&	Gold 6148	&	E5-4620v2	\\ \hline
core GHz		&	1.975	&	2.4	&	2.6 \\ \hline
core Gflops	&	31.6	&	〜30	\\ \hline
\#core/cpu*	&	16	&	20	&	8	\\ \hline
L1\$ size(D,I)		&	64KB, 64KB	&	32KB, 32KB	\\ \hline
L1D\$ BW GB/s	&	140/R+70/W	&	154/R + 77/W	\\ \hline
\$ Linesize 	&	256B	&	64B	&	64B	\\ \hline
L2\$ size		&	-	&	1MB	&	256KB	\\ \hline
L2\$ BW GB/s/c	&	-	&	154 ( ~70)	\\ \hline
LL\$ size		&	12MB	&	28MB	&	20 MB	\\ \hline
LL\$ BW GB/s/c	&	70/R+35/W	&	77 ( ~43)	\\ \hline
Memory			&	HMC(8x16Ls)	&	DDR4-2666	& DDR3-1600	\\ \hline
Mem GB/s/cpu*	&	120/R+120/W	&	128	\\ \hline
\#cores/cpu*	&	16	&	20	\\ \hline
\multicolumn{4}{l}{\scriptsize\hspace{5mm} remark. cpu* indicates processor or
CMG }\\
\end{tabular}
\fi

\end{table}


\subsection{evaluation of basic kernels}

A basic kernel composed of four basic arithmetic operations and square root
is evaluated first.
Fortran source program is shown below.


%	\begin{lstlisting}[label={listing3},captionpos=t]
%	\begin{lstlisting}[caption={\hfill},label={listing3},captionpos=t]
\begin{lstlisting}
subroutine sub_add(a,b,c,n)
real a(n), b(n), c(n)  
do i=1,n
c(i)=a(i)+b(i)
end do
return
subroutine sub_fma(a,b,c,n)
real a(n), b(n), c(n)  
do i=1,n
c(i)=a(i)+b(i)*d
end do
return
subroutine sub_divide(a,b,c,n)
real a(n), b(n), c(n)  
do i=1,n
c(i)=b(i)/a(i)
end do
return
subroutine sub_sqrt(a,b,c,n)
real a(n), b(n), c(n)  
do i=1,n
c(i)=sqrt(a(i))
end do
return
\end{lstlisting}



%%%%%%%%%%%%%%%%%%%% Restart from HERE  %%%%%%%%%%%%%%%%%

どの計算もストライド１の依存性のないループで行われるため，
効率の良いパイプライン化・SIMD化・ベクトル化処理が適用されると
期待されるものである．

上記ソースの各サブルーチンをメインプログラムから多数回呼び出して，
その平均値を得てから，サブルーチン呼び出し
オーバーヘッドの時間を差し引いて，計算に必要な時間を求める．
doループのオーバーヘッドは差し引かない．
占有状態で負荷変動が少ない小規模構成の環境で測定を行う場合は，
上記のような単純な方法で測定して問題ないが，
利用負荷が高い共用サーバなどの上で同様の結果を得るためには，
一般にOS，共有ファイルシステム，トポロジを共有するネットワーク，
などの様々な要因から時間変動があり，粒度の小さな計算において
そのようなノイズを除去することは容易ではない．
測定におけるノイズ成分を利用者側で除去する方法としては，
上記の多数回呼び出しループの外側にさらに10回程度反復する最外側ループを設け，
最外側10回測定値の内，標準偏差1σの範囲だけを採用する方法などが効果的である．

この演算カーネルの測定をループ長
\begin{math}
n=1,2,4,..,2^{26}
\end{math}
に対してFX100で実行した結果を図 \ref{fig:fx100-gflops-long-R8}に示す．
\begin{figure}[bt]
\centering\includegraphics[width=0.45\textwidth]{figs/fx100-gflops-long-R8}
\caption{fx100-gflops-long-R8}
\label{fig:fx100-gflops-long-R8}
\end{figure}

表示しているのは数値計算上の性能である．
nの増加に連れてなだらかに性能が向上し，L1キャッシュ容量に収まる範囲で
最も高い性能に達し，その後L2キャッシュ容量の範囲内で，ついでメモリ
からのバンド幅で律速されるラインに落ち着くという全体傾向は，
従来の多数の関連研究結果と重なるものであり，
ループ長が長い部分においては roofline envelopの
L1\$ / L2\$ / LLC / メモリ各階層のバンド幅で段階的に律速されている．
ループ長が短い部分においてはループ処理自体のオーバーヘッドが
演算量に比して重く，アクセスレイテンシで律速されている．
\cite{Williams:2009:RIV:1498765.1498785} 

同じカーネルに対してループ長が短い部分で
nを連続的に変化させて測定した結果を
FX100 について図 \ref{fig:fx100-gflops-short-R8}に，
IVY について図 \ref{fig:ivy-gflops-short-R8}に各々示す．
nを1から128の範囲で連続的に変化させて測定した結果と
1から50の範囲をクローズアップした結果を示している．

\begin{figure}[bt]
\centering\includegraphics[width=0.45\textwidth]{figs/fx100-gflops-short-R8}
\caption{fx100-gflops-short-R8}
\label{fig:fx100-gflops-short-R8}
\end{figure}

\begin{figure}[bt]
\centering\includegraphics[width=0.45\textwidth]{figs/ivy-gflops-short-R8}
\caption{ivy-gflops-short-R8}
\label{fig:ivy-gflops-short-R8}
\end{figure}

Perfが単調に増加するのではなく，nが一定値の倍数において規則的に
向上する変動がみられるが，これは連続データに対して
SIMD化命令を適用した計算でよくみられる状況である．
この一定値はSIMD幅（SIMDビット数/データビット数）に対応するもので，
例えばFX100において倍精度（64ビット）計算を行う場合は
\begin{math}
256 / 64 = 4
\end{math}
であるため，計算性能は
4n,4n+1,4n+2,4n+3と低下した後，4(n+1)で再び向上する．

図 \ref{fig:fx100-gflops-short-R8}は
SIMD機構をもつシステム上でのプログラミングにおいては，
プログラムのループ長を該当アーキのSIMD幅の倍数となるような
コーディングを意識することが，
短いループ長の計算性能の向上においては特に重要であることを
示している．

単精度（32ビット）計算を行う場合はこの一定値は
\begin{math}
256 / 32 = 8
\end{math}
となり，さらに顕著な性能影響効果がみられる．
図 \ref{fig:fx100-gflops-short-R4}はFX100，
図 \ref{fig:HAS-gflops-short-R4}はHaswell CPUでの単精度計算
の性能状況を示している．

\begin{figure}[bt]
\centering\includegraphics[width=0.45\textwidth]{figs/fx100-gflops-short-R4}
\caption{fx100-gflops-short-R4}
\label{fig:fx100-gflops-short-R4}
\end{figure}


\begin{figure}[bt]
\centering\includegraphics[width=0.45\textwidth]{figs/HAS-gflops-short-R4}
\caption{HAS-gflops-short-R4}
\label{fig:HAS-gflops-short-R4}
\end{figure}

PMlibを用いたプログラムの統計情報の出力および可視化によって、
このような性能特性の把握が可能となり、計算の高速化への指針の
一助が得られる。



\if0
\subsection{STREAM性能の評価}
STREAMベンチマーク\cite{stream:1995}
はコンピュータシステムのメモリバンド幅を測定するためのツールとして広く用いられている．
STREAMは変数配列の積和算（TRIAD）など，メモリread/write処理が主要な負荷となる
計算式の性能をアプリケーションプログラムレベルで測定出力する．
したがってその結果は「計算科学的観点」で評価された性能である．
このSTREAMプログラムを「システム評価的観点」で評価するとかなり様相が異なって来る．

まずSTREAMをSkylakeサーバ上で実行した場合の出力結果を示す．
Intelコンパイラのデフォルトオプションを用いている．
STREAM FortranプログラムOpenMPスレッド並列版を
1CPU上で8スレッド実行した場合，20スレッド実行した場合について示す．


Skylakeサーバ
されたHWPCベースでの
PMlibを用いて
メモリ階層での実測イベントベースによるデータ移動の状況を示す．
図



Intelコンパイラにはオプションが多数あり，中でも特にメモリバンド幅に関係が深いと
思われる以下のオプションを組み合わせて比較した結果を
図\ref{fig:stream-ivy-compact-1cpux8} に示す．\\
{\color{blue}この図は仮置きでIvybridgeの結果．Skylakeの結果と差し替えること．}

\begin{figure}[bt]
\centering\includegraphics[width=0.45\textwidth]{figs/stream-ivy-compact-1cpux8.png}
\caption{stream-ivy-compact-1cpux8}
\label{fig:stream-ivy-compact-1cpux8}
\end{figure}


配列のファーストタッチなどデータの局所性確保，スレッドのコア固定など
を注意深く行うと，ノードに搭載された４CPUの全コアを使用した場合でも
同じような傾向の結果が得られる．
図\ref{fig:stream-ivy-scatter-4cpux8} に示す．\\

\begin{figure}[bt]
\centering\includegraphics[width=0.45\textwidth]{figs/stream-ivy-scatter-4cpux8.png}
\caption{stream-ivy-scatter-4cpux8}
\label{fig:stream-ivy-scatter-4cpux8}
\end{figure}

これに対してアプリケーションのスレッド並列度が低い場合は様子が変わり，
例えばCPUあたり2スレッド（２コア）が実行されると
図\ref{fig:stream-ivy-scatter-4cpux2} の様になる．\\

\begin{figure}[bt]
\centering\includegraphics[width=0.45\textwidth]{figs/stream-ivy-scatter-4cpux2.png}
\caption{stream-ivy-scatter-4cpux2}
\label{fig:stream-ivy-scatter-4cpux2}
\end{figure}

図\ref{fig:stream-ivy-compact-1cpux8} と
図\ref{fig:stream-ivy-scatter-4cpux2} と
でPMlibが出力するHWPC Cache関係の数値を比較すると＊＊＊
が読み取れる．
\fi


\section{謝辞}
本論文の一部は，文部科学省「特定先端大型研究施設運営費等補助金（次世代超高速電子計算機システムの開発・整備等）」で実施された内容に基づくものである．

%\cite{webpage2} \par
%\cite{webpage3} \par

\bibliographystyle{jplain}
\bibliography{main}
%\bibliography{bibsample}
\end{document}

